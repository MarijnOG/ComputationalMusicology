---
title: "Index"
author: "Marijn Oude Groeneger"
date: "2024-02-21"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    orientation: rows
    social: menu
    source: embed
---

```{r setup, include=FALSE}

library(tidyverse)
library(spotifyr)
library(ggplot2)
library(plotly)
library(compmus)


# My playlist: https://open.spotify.com/playlist/2xMkwo39GHbC6An1SLv3A0?si=678c0e78f22a43f1
corpus <- get_playlist_audio_features("", "2xMkwo39GHbC6An1SLv3A0?si=38e568f8d86c48a3")
beatles <- get_playlist_audio_features("", "37i9dQZF1DZ06evO2iBPiw")


corpus_relevant <- corpus[c('track.name', 'danceability', 'instrumentalness', 'energy', 'tempo', 'track.artists')]
beatles_relevant <- beatles[c('track.name', 'danceability', 'instrumentalness', 'energy', 'tempo', 'track.artists')]


#This was done with the help of ChatGPT
corpus_relevant$artist <- lapply(corpus_relevant$track.artists, function(artist) artist$name)
corpus_relevant <- corpus_relevant[, !(names(corpus_relevant) %in% c('track.artists'))]

beatles_relevant$artist <- lapply(beatles_relevant$track.artists, function(artist) artist$name)
beatles_relevant <- beatles_relevant[, !(names(beatles_relevant) %in% c('track.artists'))]
```

Temporary First Page
================================================
Row {data-height=200}
------------------------------------------------
A recurrent theme in post rock is repetition itself, as previously mentioned. A nice example thereof is the track "Frozen Twilight" by God is an Astronaut. As this track is over six minutes long and features several sections, one would expect to be able to recognize these when compared to itself at a different time by using a Self-Similarity Matrix (SSM).

Row
------------------------------------------------

### Frozen Twilight SSM for Chroma and Timbre
```{r Frozen Twilight SSM, fig.width=12, fig.height=6}
# Taken from the example source code provided for this course.
frozenTwilight <-
  get_tidy_audio_analysis("6yMlHBgwnIhrF2Qvnt2Yyn") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )

plot_gg <- bind_rows(
  frozenTwilight |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  frozenTwilight |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  facet_wrap(~type) +
  coord_fixed() +
  scale_fill_viridis_c(option = "E", guide = "none") + 
  labs(x = "Time (s)", y = "Time (s)")


# Convert ggplot to plotly
plot_ly_object <- ggplotly(plot_gg)

# Show the interactive plot
plot_ly_object
```

Row {data-height=100}
------------------------------------------------
Interestingly, the timbre of the song remains much the same throughout the song.

```{r, include=FALSE}
TG_CampAdventure <-
  get_tidy_audio_analysis("1zvAP1uHLSDblWvv25rtad") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
M_CampAdventure <-
  get_tidy_audio_analysis("7lu0eXWcw190GRCb98U9Q8") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
dist_CampAdventure <-
  compmus_long_distance(
    TG_CampAdventure |> mutate(pitches = map(pitches, compmus_normalise, "manhattan")),
    M_CampAdventure |> mutate(pitches = map(pitches, compmus_normalise, "manhattan")),
    feature = pitches,
    method = "aitchison"
  )
```

```{r, include=FALSE}
campAdventure <-
  dist_CampAdventure |>
  mutate(
    TG_CampAdventure = xstart + xduration / 2,
    M_CampAdventure = ystart + yduration / 2
  ) |>
  ggplot(
    aes(
      x = TG_CampAdventure,
      y = M_CampAdventure,
      fill = d
    )
  ) +
  geom_tile(aes(width = xduration, height = yduration)) +
  coord_fixed() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  labs(x = "Twin Galaxies", y = "Management")
campAdventure
```


Introduction
================================================

Row {data-height=150}
------------------------------------------------
The goal of this corpus of music and the analysis thereof is to find out whether Spotify makes distinctions between different kinds of alternative 'emo' music. The styles that I am interested in include math rock, post rock and Midwest (emo). To those uninitiated in the cult, I will give a brief introduction, as well as give an example of what I consider a typical track for these styles. 

Row {data-height=700}
------------------------------------------------
### Math rock
Math rock is characterized by harmonically rich chords, often involving major sixth, all kinds of seven and nine, eleven and thirteen chords. Rhythmically, the music rarely follows a 4/4 structure, often changing time signatures throughout songs. The style of playing that is most notable in the often very present and highly technical electric guitars can be described as 'angular' with big intervals and high contrasts in syncopation, as well as accenting off-beats frequently. 

A typical track could be: Bubble Dream by Chon

### Post rock
Post rock is harmonically similar to math rock, hence why the two are often grouped together. Post rock, however, conveys a much different emotion, which I would personally describe as “positive solitude”, like being very alone and being okay with that. The style is much calmer and less ‘angular’ than math rock, and thrives on winding repetitions of somewhat meditative leitmotifs, often preceded by sonically big crescendos.

Your hand in mine by Explosions in the Sky is very post rock indeed.

### Midwest emo
This style is a bit harder to describe, as it features much of the same characteristics of the previous two styles, but in varying degrees. Midwest can be very peaceful or feature metal-esque screaming, depending on the song and artist. A common trait, one that is shared in most emo music, is yelling vocals. The previous two styles are typically instrumental, whereas this style usually offers emotional or downright depressing lyrics for the listener to enjoy or wallow in. It still features the same harmonic ideas.

Never meant by American Football is the most famous and ubiquitous example.

Row {data-height=250}
------------------------------------------------

Exploring the distinctions between math rock, post rock, and Midwest emo using the characteristics provided by the Spotify API presents an intriguing challenge. There is much debate among fans about the boundaries and overlap of these genres, hence why such an analysis could be meaningful. 
An example of an outlier could be Polyphia, which is traditionally a math rock/metal band that turned more to an electronic and hip hop influenced instrumental boy band. Where would they land on the spectrum? Through the use of the toolset provided by Spotify, new insights in this subset of emo-genres can be gained.


Dancing to the Beatles compared to my corpus
================================================

Row
------------------------------------------------
### Corpus energy metric spread
```{r Corpus energy metrics}
p1 <- plot_ly(corpus_relevant, x = ~danceability, y = ~tempo, color = ~energy, type = 'scatter', mode = 'markers', text = ~track.name) %>%
  layout(xaxis = list(title = "Danceability score", range = c(0, 1)),
         yaxis = list(title = "Tempo (bpm)", range = c(80, 210)),
         showlegend = TRUE)

p1

#TODO Indicate outliers and their genre (mostly Midwest)
```

### Beatles (best of) energy metrics spread
```{r Beatles energy metrics}
p2 <- plot_ly(beatles_relevant, x = ~danceability, y = ~tempo, color = ~energy, type = 'scatter', mode = 'markers', text = ~track.name) %>%
  layout(xaxis = list(title = "Danceability score", range = c(0, 1)),
         yaxis = list(title = "Tempo (bpm)", range = c(80, 210)),
         showlegend = TRUE)

p2
```

Row {data-height=100}
------------------------------------------------
We can observe that my selected styles of music would serve marginally less well in a dancing hall as compared to the Beatles on average, according to Spotify that is. This also reveals a bit of the bias of this danceability feature, as some of the tracks in my corpus would do fine in, for instance, a more meditative style of dancing. The Beatles also score a bit lower than I would expect, since they have released numerous radio and dance venue hits. This, however, seems to be the nature of this feature. Martin Garrix for example, a renowned Dance DJ/producer, averages around 0.6 'danceability' when looking at his five most popular songs.


Does Polyphia make Light work, or rather LIT?
================================================

Row
------------------------------------------------
### Chromagram of the Polyphia track "Light"
```{r Polyphia Light}
Light <- get_track_audio_analysis("3YMT7YnliDwUzG3FL4y4kq")
Light_filtered <- Light$segments

one_through_12 = data.frame(chroma = rep(1:12, length.out = ))

Light_filtered <- Light_filtered[, c(1, 8)] %>% 
    unnest(cols = pitches)
Light_filtered = cbind(Light_filtered, one_through_12)

# Create a heatmap
p3 <- plot_ly(Light_filtered, x = ~factor(start), y = ~factor(chroma), z = ~pitches, type = "heatmap") %>%
  colorbar(title = "Pitches", tickvals = seq(min(Light_filtered$pitches), max(Light_filtered$pitches), length.out = 5), tickformat = ".2f") %>%
  layout(title = "Chromagram for Light", xaxis = list(title = "Segment start"), yaxis = list(title = "Chroma feature"))

p3
```

### Chromagram of the Polyphia track "LIT"
```{r Polyphia LIT}
LIT <- get_track_audio_analysis("3RoycW4yhd2HCsWmLR7xIi")
LIT_filtered <- LIT$segments

LIT_filtered <- LIT_filtered[, c(1, 8)] %>% 
    unnest(cols = pitches)
LIT_filtered = cbind(LIT_filtered, one_through_12)

# Create a heatmap
p4 <- plot_ly(LIT_filtered, x = ~factor(start), y = ~factor(chroma), z = ~pitches, type = "heatmap") %>%
  colorbar(title = "Pitches", tickvals = seq(min(LIT_filtered$pitches), max(LIT_filtered$pitches), length.out = 5), tickformat = ".2f") %>%
  layout(title = "Chromagram for LIT", xaxis = list(title = "Segment start"), yaxis = list(title = "Chroma feature"))

p4
```

Row {data-height=250}
------------------------------------------------
These are two versions of the same song, both released by the band Polyphia. The original version, 'Light', features a lot more of the band's original sound, with technical intertwining guitar parts and prog-rock-esque rhythms. In the rerelease, called 'LIT', the band opts for a strongly electronic sound. While the electric guitars are still clearly distinguishable, it is obvious that a lot of studio work has gone into this version of the track. It also features typical elements of dance music, such as opening and closing of filters and compressors, a strong bass drum and background effects to fill up the sound. This last part is interesting, as the chromagram appears a bit more smeared or less distinct in the electronic version. Perhaps these smears over the spectrum are these very effects, as they are often not clearly pitched. Other than that, there appear not to be many major differences.
